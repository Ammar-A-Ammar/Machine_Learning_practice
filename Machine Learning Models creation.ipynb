{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ammar-A-Ammar/Machine_Learning_projects/blob/main/Machine%20Learning%20Models%20creation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa-qzF5aXD_v"
      },
      "source": [
        "---\n",
        "# Cairo University Faculty of Engineering\n",
        "## Data Mining and Machine Learning\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXbX0rUsXD_y"
      },
      "source": [
        "## Instructions\n",
        "- Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
        "- Don't change any of the cells.\n",
        "- If you add cells make sure to remove them before you submit\n",
        "- ONLY fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", \n",
        "- Fill in your **full name** below inside \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NhcMa5pXD_y"
      },
      "outputs": [],
      "source": [
        "NAME = \"Ammar Ahmed Ammar\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qV5aRdYXD_z"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f98e3efbfba32ad745adf0576f22d0b0",
          "grade": false,
          "grade_id": "cell-ba116f7a3e5dde9a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "An6xtCk3XD_0"
      },
      "outputs": [],
      "source": [
        "#Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression \n",
        "import copy\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error as mse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "7afe45862586d7f703831eef0c1520c3",
          "grade": false,
          "grade_id": "cell-1499d4be2b89b9b5",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "743SbTotXD_0"
      },
      "source": [
        "---\n",
        "# Part 1 - Data\n",
        "\n",
        "For this assignment, you will be using the [Diabetes](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset) dataset to create a regression model that can help diagnose patients' disease progression one year after baseline.\n",
        "\n",
        "\n",
        "## Question 1 (1 point)\n",
        "\n",
        "Load Diabetes Dataset from [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn-datasets-load-diabetes)\n",
        "\n",
        "The following function MUST:\n",
        "1. Load diabetes dataset from scikit-learn as a scikit-learn bunch object which is similar to a dictionary\n",
        "    - Use `scaled= False`\n",
        "2. Convert the diabetes data to a pandas dataframe\n",
        "3. Return a (569, 31) pandas dataframe with **CORRECT COLUMN NAMES**, target column should be named \"target\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "faa40ad41c07da6fb4f0916b8368955c",
          "grade": false,
          "grade_id": "cell-e5ed3e296d524dfa",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "id": "48hpc3egXD_1"
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "    \"\"\"\n",
        "    Return a pandas dataframe    \n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    #data = load_diabetes(as_frame=True)\n",
        "    #data=load_diabetes( return_X_y=False, as_frame=True, scaled=False)\n",
        "    data=load_diabetes( as_frame=True, scaled=False)\n",
        "    df = pd.DataFrame(data=data.data, columns=data.feature_names)\n",
        "    df['target'] = data.target\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e42ea6dc1194dee9a3d4d08c49897f49",
          "grade": true,
          "grade_id": "cell-5cc537b68d837655",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "f0fkGQyHXD_1"
      },
      "outputs": [],
      "source": [
        "#diabetes = load_data() #submit with this line commented\n",
        "#print(diabetes.head()) #submit with this line commented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "d009431de111df406154cc1035793b99",
          "grade": false,
          "grade_id": "cell-081b548a088ef1cb",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "j0nn3E5pXD_1"
      },
      "source": [
        "## Question 2\n",
        "\n",
        "Print a data description of all columns of dataframe \n",
        "\n",
        "use `.describe()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8e2d097e87e5490400a3917aa9e12bf6",
          "grade": false,
          "grade_id": "cell-6003a34ecd262271",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "id": "8D8iIiwNXD_2"
      },
      "outputs": [],
      "source": [
        "def range_columns(df):\n",
        "    \"\"\"\n",
        "    Function should return the drscription of dataframe\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    #data=load_diabetes(return_X_y=False, as_frame=True, scaled=False)\n",
        "    #df = pd.DataFrame(data=data.data, columns=data.feature_names)\n",
        "    #df1=df.describe()\n",
        "\n",
        "    return df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8aba752437ce3122bb1d35224c5d79bf",
          "grade": true,
          "grade_id": "cell-11ebb07531f176f6",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "-IWqiKnuXD_2"
      },
      "outputs": [],
      "source": [
        "#print(range_columns(diabetes)) #submit with this line commented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "648cf1fec1aec45e3dbac3102cdd65a4",
          "grade": false,
          "grade_id": "cell-21c2ec7b648d3660",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "gdfuJtgfXD_2"
      },
      "source": [
        "## Question 3\n",
        "\n",
        "Remove all categorical features from dataframe. Function should return the new dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "56b057abf7fcd3aae291faa9b18f4b8f",
          "grade": false,
          "grade_id": "cell-789de9d85f97ed27",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "id": "fuW9SXRRXD_2"
      },
      "outputs": [],
      "source": [
        "def drop_categorical(df):\n",
        "    \"\"\"\n",
        "    Function should return the dataframe with categorical features removed\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    #data=load_diabetes( as_frame=True, scaled=False)\n",
        "    #df = pd.DataFrame(data=data.data, columns=data.feature_names)\n",
        "    df.drop('sex',axis=1 , inplace = True)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5476307d846bb3ee83fc24f2fa335c54",
          "grade": true,
          "grade_id": "cell-0b738f50b27a41fc",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "VuJbnIn1XD_3"
      },
      "outputs": [],
      "source": [
        "#print(drop_categorical(diabetes).columns)  #submit with this line commented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "5c43000443ad3c3444df365fbf4a2ef9",
          "grade": false,
          "grade_id": "cell-15c9a87c0d4f4077",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "bFA4MJ3yXD_3"
      },
      "source": [
        "## Question 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "575a98db586d72da5599d8780c3514d9",
          "grade": false,
          "grade_id": "cell-d8d8657734adf2b1",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "leqwZRjIXD_3"
      },
      "source": [
        "Using `train_test_split`, split `X` and `y` into training and test sets. With 80% training and 10% test.\n",
        "\n",
        "USE `random_state=0`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ce80d19b77f5b50d02e00146b2ebb744",
          "grade": false,
          "grade_id": "cell-a119ec96cd9cb272",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "id": "GzNFE_XdXD_3"
      },
      "outputs": [],
      "source": [
        "def split(df):\n",
        "    \"\"\"\n",
        "    Function should return X_train, X_test, y_train, y_test \n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    x=df.drop('target',axis=1 , inplace = False)\n",
        "    y=df.target\n",
        "    df_split=train_test_split(x,y,test_size=0.1, train_size=0.8, random_state=0, shuffle=True)\n",
        "    \n",
        "    #df.model_selection.train_test_split(test_size=0.1, train_size=0.8, random_state=0, shuffle=True, stratify=None)\n",
        "    return df_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "03cb19b01b95d1346f0abe5ee2a855c9",
          "grade": true,
          "grade_id": "cell-15bb9f41e4f93fda",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "0g0WCoSJXD_3",
        "outputId": "2acb1fe9-eebf-4397-83a2-82c335e292c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nX_train, X_test, y_train, y_test = split(diabetes) #submit with this line commented\\n#print(X_train, X_test, y_train, y_test)\\nprint(\"X_train shape:\", X_train.shape)\\nprint(\"y_train shape:\", y_train.shape)\\nprint(\"X_test shape:\", X_test.shape)\\nprint(\"y_test shape:\", y_test.shape)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 134
        }
      ],
      "source": [
        "\"\"\"\n",
        "X_train, X_test, y_train, y_test = split(diabetes) #submit with this line commented\n",
        "#print(X_train, X_test, y_train, y_test)\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "80a91dc180c71eb97e738f213102a1eb",
          "grade": false,
          "grade_id": "cell-8693011285601e24",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "Qe_wLL4iXD_4"
      },
      "source": [
        "## Question 5\n",
        "\n",
        "Use scikit-learn standard scaler to standardize training data into 0 mean and 1 std.\n",
        "\n",
        "Function must return the normalized data and the standard scaler object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3c731ea29ef6a6982dcdf1a369283f02",
          "grade": false,
          "grade_id": "cell-4a35276938e7e20d",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "id": "XZmoTlQQXD_4"
      },
      "outputs": [],
      "source": [
        "def standardize(data):\n",
        "    \"\"\"Standardize a data to 0 mean and 1 std\n",
        "    Return standardized data AND the scaler object\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    X_std = scaler.fit_transform(data)\n",
        "    return X_std, scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1d5e65444178d29b2e8f47e9a5a5bec7",
          "grade": true,
          "grade_id": "cell-07f946868d8ceacd",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "_epHKmJeXD_4"
      },
      "outputs": [],
      "source": [
        "#scaled_X_train, x_scaler = standardize(X_train)  #submit with this line commented\n",
        "#scaled_y_train, y_scaler = standardize(y_train.to_numpy()[:,np.newaxis])  #submit with this line commented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ba9b97214e708732789979a6abea9a0e",
          "grade": false,
          "grade_id": "cell-dc55cf0e2aa1487f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WuXuhTzXD_4",
        "outputId": "01f579ee-462b-4da6-982f-ee06e950eca0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(353, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ],
      "source": [
        "# We must apply the same transformations to the test data\n",
        "scaled_X_test = x_scaler.transform(X_test)\n",
        "scaled_y_test = y_scaler.transform(y_test.to_numpy()[:, np.newaxis])\n",
        "scaled_y_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "74376db903473d88354d43552ac28f8e",
          "grade": false,
          "grade_id": "cell-86e14a5ac3dd7f3a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "AsZoeTEoXD_4"
      },
      "source": [
        "# Part 2 - Regression from scratch\n",
        "We will fit a linear regression model parameters $(w,b)$ to our dataset.\n",
        "- The model function for linear regression, which is a function that maps from `x` to `y` is represented as \n",
        "    $$f_{w,b}(x) = \\sum{wx} + b$$\n",
        "- For one variable, the cost function for linear regression $J(w,b)$ is defined as\n",
        "\n",
        "$$J(w,b) = \\frac{1}{2n} \\sum\\limits_{i = 0}^{n-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2$$ \n",
        "\n",
        "- $n$ is the number of training examples in the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "995bccf763fc7c90af00c99e6bbcf9ef",
          "grade": false,
          "grade_id": "cell-83773e0de9f94233",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "r2R3OFoSXD_4"
      },
      "source": [
        "## Question 1 Cost\n",
        "\n",
        "The `compute_cost` should:\n",
        "\n",
        "* Iterate over the training examples, and for each example, compute:\n",
        "    * The prediction of the model for that example \n",
        "    $$\n",
        "    y_{pred}^{(i)}=f_{wb}(x^{(i)}) =  \\sum_{k=1}^{k=m} w_{k}x^{(i)}_{k} + b \n",
        "    $$\n",
        "   \n",
        "    * The cost for that example  $$cost^{(i)} =  (f_{wb} - y^{(i)})^2$$\n",
        "    \n",
        "\n",
        "* Return the total cost over all examples\n",
        "$$J(\\mathbf{w},b) = \\frac{1}{2n} \\sum\\limits_{i = 0}^{n-1} cost^{(i)}$$\n",
        "  * Here, $n$ is the number of training examples and $\\sum$ is the summation operator.\n",
        "  * Here, $m$ is the number of features \n",
        "  \n",
        "**Make sure that the cost is returned as  float**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7510f73efe14a37759475407e3b76c37",
          "grade": false,
          "grade_id": "cell-8ccc1eaa5bd14381",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "id": "5VEZ8tR9XD_5"
      },
      "outputs": [],
      "source": [
        "def compute_cost(x, y, w, b): \n",
        "    \"\"\"\n",
        "    Computes the cost function for linear regression.\n",
        "    \n",
        "    Args:\n",
        "        x (ndarray): Shape (n,m) Input to the model\n",
        "        y (ndarray): Shape (n,) Label\n",
        "        w, b: Parameters of the model\n",
        "    \n",
        "    Returns\n",
        "        total_cost (float): The cost of using w,b as the parameters for linear regression\n",
        "               to fit the data points in x and y\n",
        "    \"\"\"\n",
        "    \n",
        "    xLenght = len(x)\n",
        "    cost = 0\n",
        "    for i in range(xLenght):\n",
        "        f_Respect_w = w * x[i] + b\n",
        "        cost = cost + (f_Respect_w - y[i])**2\n",
        "    total_cost = 1 / (2 * xLenght) * cost\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    \"\"\" Also Failed because gradiant decent function\n",
        "    # YOUR CODE HERE\n",
        "    x2=np.array(x)\n",
        "    n=x2.shape[0]\n",
        "    m=x2.shape[1]\n",
        "    y_pred=[]\n",
        "    for i in range(n):\n",
        "      sum=0\n",
        "      for j in range(m):\n",
        "        sum= sum + (w[j]*x2[i][j]+b)\n",
        "      y_pred.append(sum[0])\n",
        "    cost=[]  \n",
        "    for i in range(n):\n",
        "      cost.append((int(y_pred[i])-y[i])**2)  \n",
        "    cost=np.array(cost)\n",
        "    totalCost=((cost.sum())/(2*n))\n",
        "\n",
        "    return totalCost\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    def custom_mult(x,w):\n",
        "\t    return np.matmul(x, w)\n",
        "     \n",
        "def compute_cost8(x, y, w,b):\n",
        "    \n",
        "    cost=sum(((custom_mult(x, w)-y).T@(custom_mult(x, w)-y))/(2*y.shape[0]))/len(((custom_mult(x, w)-y).T@(custom_mult(x, w)-y))/(2*y.shape[0]))\n",
        "    cost2=sum(cost)/(2*len(cost))\n",
        "    #return cost\n",
        "    #print(cost)\n",
        "    return cost\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    def hypothesis(x,w):\n",
        "    return np.matmul(w,x)\n",
        "def cost_calc( x, y, w, b):\n",
        "    return (1/2*w) * np.sum((hypothesis(w, x) - y)**2)\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    def costfunction4(x,y,w,b):\n",
        "  m=len(x)              \n",
        "  s=np.power((np.dot(x,w))-y,2)\n",
        "  #s=s[:int(len(s)/4)]\n",
        "  J=np.sum(s)/(2*m)\n",
        "  return s\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "  def compute_cost_zft(x, y, w, b): \n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    x2=np.array(x)\n",
        "    n=x2.shape[0]\n",
        "    m=x2.shape[1]\n",
        "    y_pred=[]\n",
        "    for i in range(n):\n",
        "      sum=0\n",
        "      for j in range(m):\n",
        "        sum= sum + (w[j]*x2[i][j]+b)\n",
        "      y_pred.append(sum[0])\n",
        "    cost=[]  \n",
        "    for i in range(n):\n",
        "      cost.append((int(y_pred[i])-y[i])**2)  \n",
        "    cost=np.array(cost)\n",
        "    totalCost=((cost.sum())/(2*n))\n",
        "\n",
        "    return cost\n",
        "##\n",
        "    def h(x,theta):\n",
        "\n",
        "\t    return np.matmul(x, theta)\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5b880ddb06055c9ebbafeb9635b8e33f",
          "grade": true,
          "grade_id": "cell-e8c5fe61dffa5a35",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "93BT21TLXD_5"
      },
      "outputs": [],
      "source": [
        "#total_cost = compute_cost(X_train, y_train.to_numpy(), w=np.zeros((X_train.shape[1],1)), b=0)#submit with this line commented\n",
        "#print (total_cost)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a2e5677e11338472449478aa5c179864",
          "grade": false,
          "grade_id": "cell-bbe314c5ce7f8f0c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "PRYYsEtIXD_5"
      },
      "source": [
        "## Question 2 Gradient\n",
        "\n",
        "The gradient descent algorithm is:\n",
        "\n",
        "$$\\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & \\phantom {0000} b := b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b} \\newline       \\; & \\phantom {0000} w_k := w_k -  \\alpha \\frac{\\partial J(w,b)}{\\partial w_k} \\tag{1}  \\; & \n",
        "\\newline & \\rbrace\\end{align*}$$\n",
        "\n",
        "where, parameters $w, b$ are both updated simultaniously and where  \n",
        "$$\n",
        "\\frac{\\partial J(w,b)}{\\partial b}  = \\frac{1}{n} \\sum\\limits_{i = 0}^{n-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{2}\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial J(w,b)}{\\partial w_k}  = \\frac{1}{n} \\sum\\limits_{i = 0}^{n-1} (f_{w,b}(x^{(i)}) -y^{(i)})x^{(i)}_k \\tag{3}\n",
        "$$\n",
        "    \n",
        "*  $f_{w,b}(x^{(i)})$ is the model's prediction, while $y^{(i)}$, is the target value\n",
        "\n",
        "\n",
        "You will implement a function called `compute_gradient` which calculates $\\frac{\\partial J(w)}{\\partial w_k}$, $\\frac{\\partial J(w)}{\\partial b}$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "cdba2775323e87bf365e34deb029cf44",
          "grade": false,
          "grade_id": "cell-c7e63ff942a0d2ed",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "UGJaOlrkXD_5"
      },
      "source": [
        "The `compute_gradient` function should:\n",
        "\n",
        "* Iterate over the training examples, and for each example, compute:\n",
        "    * The prediction of the model for that example \n",
        "    $$\n",
        "    f_{wb}(x^{(i)}) =  \\sum_{k=1}^{k=m}w_kx^{(i)} + b \n",
        "    $$\n",
        "   \n",
        "    * Loop over features $k$ and calculate the gradient for the parameters $w_k$\n",
        "        $$\n",
        "        \\frac{\\partial J(w,b)}{\\partial w_k}^{(i)}  =  (f_{w,b}(x^{(i)}) -y^{(i)})x^{(i)}_k \n",
        "        $$\n",
        "    * Then for $b$ from that example \n",
        "        $$\n",
        "        \\frac{\\partial J(w,b)}{\\partial b}^{(i)}  =  (f_{w,b}(x^{(i)}) - y^{(i)}) \n",
        "        $$\n",
        "\n",
        "* Return the total gradient update from all the examples for each parameter $w_k$ and $b$\n",
        "    $$\n",
        "    \\frac{\\partial J(w,b)}{\\partial b}  = \\frac{1}{n} \\sum\\limits_{i = 0}^{n-1} \\frac{\\partial J(w,b)}{\\partial b}^{(i)}\n",
        "    $$\n",
        "    \n",
        "    $$\n",
        "    \\frac{\\partial J(w,b)}{\\partial w_k}  = \\frac{1}{n} \\sum\\limits_{i = 0}^{n-1} \\frac{\\partial J(w,b)}{\\partial w_k}^{(i)} \n",
        "    $$\n",
        "  * Here, $n$ is the number of training examples and $\\sum$ is the summation operator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f181801dabc9b977abf7d770dd405ba1",
          "grade": false,
          "grade_id": "cell-90331fc630220c79",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "id": "PntbedOIXD_5"
      },
      "outputs": [],
      "source": [
        "def compute_gradient(x, y, w, b): \n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression \n",
        "    Args:\n",
        "      x (ndarray): Shape (n,m) Input to the model \n",
        "      y (ndarray): Shape (n,) Label \n",
        "      w, b : Parameters of the model (w array, b scalar)  \n",
        "    Returns\n",
        "      dj_dw (ndarray): The gradients of the cost w.r.t. the parameters w\n",
        "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b     \n",
        "     \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    xLenght = len(x)   \n",
        "    j_Respect_w = 0\n",
        "    j_Respect_b = 0\n",
        "    \n",
        "    for i in range(xLenght):  \n",
        "        f_Respect_b = w * x[i] + b \n",
        "        j_Respect_w_i = (f_Respect_b - y[i]) * x[i] \n",
        "        j_Respect_b_i = f_Respect_b - y[i] \n",
        "        j_Respect_b += j_Respect_b_i\n",
        "        j_Respect_w += j_Respect_w_i \n",
        "    j_Respect_w = j_Respect_w / xLenght \n",
        "    j_Respect_b = j_Respect_b / xLenght \n",
        "        \n",
        "    return j_Respect_w, j_Respect_b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "eebe40c14c94f2ee8e4ad2d3259702d9",
          "grade": true,
          "grade_id": "cell-3de637ff5e0eaa27",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "q5zGubpyXD_6"
      },
      "outputs": [],
      "source": [
        "#grad_w, grad_b = compute_gradient(X_train.to_numpy(), y_train.to_numpy(), w=np.zeros((X_train.shape[1],1)), b=0)#submit with this line commented\n",
        "#print(grad_w)\n",
        "#print(grad_b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "ad23491b5e42465528c0c774f7547986",
          "grade": false,
          "grade_id": "cell-66feae8f792d201d",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "NncwUU-TXD_6"
      },
      "source": [
        "## Question 3 Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "2299df70cfeaa35c5c6ae0ef0cab717e",
          "grade": false,
          "grade_id": "cell-bc97c2e75a6f32c1",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "lo3x0u4iXD_6"
      },
      "source": [
        "We will now find the optimal parameters of a linear regression model by using batch gradient descent. Batch refers to running all the examples in one iteration.\n",
        "\n",
        "- A good way to verify that gradient descent is working correctly is to look at the value of $J(w,b)$ and check that it is decreasing with each step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "556c6cb838b6f0403db1e1562d707d2c",
          "grade": false,
          "grade_id": "cell-7621c3bf24150795",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "vF83YBkuXD_6"
      },
      "source": [
        "- **For the following function fill in the missing code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "92147571f951b1ed2fe7ff22da1edccb",
          "grade": true,
          "grade_id": "cell-513f6d86308fc320",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "id": "YwjNqU7FXD_6"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(x, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
        "    \"\"\"\n",
        "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
        "    num_iters gradient steps with learning rate alpha\n",
        "    \n",
        "    Args:\n",
        "      x :    (ndarray): Shape (n,m)\n",
        "      y :    (ndarray): Shape (n,)\n",
        "      w_in, b_in : Initial values of parameters of the model\n",
        "      cost_function: function to compute cost\n",
        "      gradient_function: function to compute the gradient\n",
        "      alpha : (float) Learning rate\n",
        "      num_iters : (int) number of iterations to run gradient descent\n",
        "    Returns\n",
        "      w : (ndarray): Updated values of parameters of the model after\n",
        "          running gradient descent\n",
        "      b : (scalar): Updated value of parameter of the model after\n",
        "          running gradient descent\n",
        "    \"\"\"\n",
        "    \n",
        "    # number of training examples\n",
        "    m = len(x)\n",
        "    \n",
        "    # An array to store cost J and w's at each iteration â€” primarily for graphing later\n",
        "    J_history = []\n",
        "    w_history = []\n",
        "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
        "    b = b_in\n",
        "    \n",
        "    for i in range(num_iters):\n",
        "\n",
        "        # Calculate the gradient dj_db, dj_dw\n",
        "        # YOUR CODE HERE\n",
        "        j_Respect_w, j_Respect_b= gradient_function(x, y, w , b)     \n",
        "        \n",
        "        # Update Parameters using w, b, alpha and gradient\n",
        "        # YOUR CODE HERE\n",
        "        b=b-alpha*j_Respect_b                            \n",
        "        w=w-alpha*j_Respect_w                            \n",
        "        \n",
        "        if i<100000:      # prevent resource exhaustion \n",
        "            cost =  cost_function(x, y, w, b)\n",
        "            J_history.append(cost)\n",
        "\n",
        "        if i% math.ceil(num_iters/10) == 0:\n",
        "            w_history.append(w)\n",
        "            print(f\"Iteration {i:4}: Cost {J_history[-1]}\")\n",
        "        \n",
        "    return w, b, J_history, w_history #return w and J,w history for graphing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0ae8c87427da4bf62ccaf0a647ac4187",
          "grade": false,
          "grade_id": "cell-d7b0db5275087c0a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "dhJpy4skXD_6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da53f9cf-f131-4730-9094-169e2e657f01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration    0: Cost None\n",
            "Iteration  150: Cost None\n",
            "Iteration  300: Cost None\n",
            "Iteration  450: Cost None\n",
            "Iteration  600: Cost None\n",
            "Iteration  750: Cost None\n",
            "Iteration  900: Cost None\n",
            "Iteration 1050: Cost None\n",
            "Iteration 1200: Cost None\n",
            "Iteration 1350: Cost None\n",
            "w,b found by gradient descent: [[2.94654506 5.80589969 1.60849886 0.77954852 1.23588514 2.64598797\n",
            "  8.90895726 9.50641332 1.66020203]\n",
            " [2.94660629 5.80612833 1.60851673 0.77955285 1.23589595 2.64604594\n",
            "  9.03591633 9.62687197 1.66022142]\n",
            " [2.94656491 5.80597383 1.60850465 0.77954992 1.23588865 2.64600677\n",
            "  8.95012565 9.54547385 1.66020832]\n",
            " [2.94654361 5.80589429 1.60849843 0.77954842 1.23588489 2.6459866\n",
            "  8.90595809 9.50356771 1.66020157]\n",
            " [2.94649899 5.80572769 1.60848541 0.77954526 1.23587702 2.64594436\n",
            "  8.81345042 9.41579652 1.66018744]\n",
            " [2.94658079 5.8060331  1.60850929 0.77955105 1.23589145 2.64602179\n",
            "  8.9830381  9.57670115 1.66021334]\n",
            " [2.94650412 5.80574684 1.60848691 0.77954563 1.23587792 2.64594921\n",
            "  8.82408205 9.4258838  1.66018906]\n",
            " [2.94667128 5.80637099 1.6085357  0.77955744 1.23590742 2.64610747\n",
            "  9.17066481 9.75472119 1.66024201]\n",
            " [2.94669773 5.80646979 1.60854342 0.77955932 1.23591208 2.64613252\n",
            "  9.22552287 9.80677047 1.66025039]] [[0.53025251 0.52494238 0.36488639 0.41420669 0.48770308 0.67654075\n",
            "  2.31624753 2.25006702 0.37389396]\n",
            " [0.52706295 0.51882616 0.36316229 0.41335817 0.48636309 0.67343711\n",
            "  2.30744031 2.24027453 0.37209794]\n",
            " [0.52921825 0.5229591  0.36432732 0.41393154 0.48726857 0.67553435\n",
            "  2.31339166 2.24689166 0.37331157]\n",
            " [0.53032786 0.52508686 0.36492712 0.41422673 0.48773474 0.67661407\n",
            "  2.31645559 2.25029834 0.37393638]\n",
            " [0.53265191 0.52954339 0.36618337 0.41484499 0.48871111 0.67887551\n",
            "  2.3228729  2.25743355 0.37524504]\n",
            " [0.5283914  0.52137355 0.36388037 0.41371158 0.48692119 0.67472977\n",
            "  2.3111085  2.24435308 0.37284598]\n",
            " [0.53238481 0.52903121 0.366039   0.41477394 0.4885989  0.67861561\n",
            "  2.32213537 2.25661353 0.37509464]\n",
            " [0.5236777  0.5123347  0.3613324  0.4124576  0.48494088 0.67014305\n",
            "  2.29809274 2.22988125 0.37019174]\n",
            " [0.52229951 0.50969193 0.36058743 0.41209096 0.48436188 0.66880199\n",
            "  2.2942872  2.22564999 0.36941569]]\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(0)\n",
        "# initialize fitting parameters. Recall that the shape of w is (m,)\n",
        "w = np.random.rand(X_train.shape[1],1)\n",
        "b = np.random.rand(1)\n",
        "\n",
        "# some gradient descent settings\n",
        "iterations = 1500\n",
        "alpha = 0.00001\n",
        "\n",
        "w,b,_,_ = gradient_descent(X_train.to_numpy() ,y_train.to_numpy(), w, b, \n",
        "                     compute_cost, compute_gradient, alpha, iterations)\n",
        "print(\"w,b found by gradient descent:\", w, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "1fb25da7143d7d0f48ad28cd0db0008f",
          "grade": false,
          "grade_id": "cell-2c81b94ee8803821",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "p6Kdmd5MXD_7"
      },
      "source": [
        "# Part 2 Scikit-Learn \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "8e67409d1906f195ae7c27a43a3293cc",
          "grade": false,
          "grade_id": "cell-fe4f701dd58af234",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "fX0WhfiGXD_7"
      },
      "source": [
        "Create a scikit-learn regression model and train it on diabetes data.\n",
        "Your function should return the trained model\n",
        "\n",
        "**Your previous results could be different from bellow model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0ab00236114e424d86156fe50e84754b",
          "grade": true,
          "grade_id": "cell-bf77c8451f998e23",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "id": "xDfaHAv3XD_7"
      },
      "outputs": [],
      "source": [
        "def regression_sc(X, y):\n",
        "    \"\"\"Trains a linear regression model on X, y data \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    lr = LinearRegression()\n",
        "\n",
        "    #fitting the model\n",
        "    lr.fit(X_train,y_train)\n",
        "    LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
        "\n",
        "    # Predicting the Test set results\n",
        "    y_pred = lr.predict(X_test)\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn import linear_model\n",
        "def regression_sc(X, y):\n",
        "    \"\"\"Trains a linear regression model on X, y data \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    #X,y = shap.datasets.diabetes()\n",
        "    #X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "    #lin_regr = linear_model.LinearRegression()\n",
        "    #lin_regr.fit(X_train, y_train)\n",
        "    train_x, test_x, train_y, test_y = train_test_split(X,y,test_size=0.2,random_state=0)\n",
        "\n",
        "    Trained_Model = LinearRegression()\n",
        "    Trained_Model.fit(train_x, train_y)\n",
        "    return Trained_Model"
      ],
      "metadata": {
        "id": "Lc7OXVZeWR42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a77bec0031c28cb095a7a3455206ac37",
          "grade": false,
          "grade_id": "cell-b04ae1fb56eeb528",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tb05a7reXD_7",
        "outputId": "2c86c6b5-d77c-443f-fdf3-3719776d5d6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "linear model coeff (w): [ 7.54341942e-03  6.77378597e+00  1.09220036e+00 -7.90413440e-01\n",
            "  5.99038641e-01  1.66327418e-01  3.14148923e+00  5.47199424e+01\n",
            "  1.81230969e-01]\n",
            "linear model intercept (b): -342.068\n",
            "linear model mse:2936.9205894458564\n",
            "linear model R^2: 0.504\n"
          ]
        }
      ],
      "source": [
        "linreg = regression_sc(X_train, y_train)\n",
        "y_pred = linreg.predict(X_train)\n",
        "\n",
        "print('linear model coeff (w): {}'.format(linreg.coef_))\n",
        "print('linear model intercept (b): {:.3f}'.format(linreg.intercept_))\n",
        "print(f'linear model mse:{mse(y_train, y_pred)}')\n",
        "print('linear model R^2: {:.3f}'.format(linreg.score(X_train, y_train)))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}